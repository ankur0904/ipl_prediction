{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries \n",
    "\n",
    "# Basic data pre-processing libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For splitting dataset between train and test set .\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# For applying the feature scaling on dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For oversampling the unbalanced dataset\n",
    "from imblearn.over_sampling import SMOTE  \n",
    "\n",
    "# Select important features from dataset\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# RandomForest Library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# XGBOOST Library \n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import optuna\n",
    "\n",
    "# Neural Network Library\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import Sequential,regularizers \n",
    "from tensorflow.keras.layers import Dense, Dropout , BatchNormalization,LeakyReLU\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "# calculating Precision , Recall and F-1 score and other tools  for model evaluation\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, roc_auc_score ,mean_squared_error,make_scorer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Blood Pressure</th>\n",
       "      <th>Heart Rate</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Family History</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>...</th>\n",
       "      <th>Sedentary Hours Per Day</th>\n",
       "      <th>Income</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Triglycerides</th>\n",
       "      <th>Physical Activity Days Per Week</th>\n",
       "      <th>Sleep Hours Per Day</th>\n",
       "      <th>Country</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Hemisphere</th>\n",
       "      <th>Heart Attack Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BMW7812</td>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>208</td>\n",
       "      <td>158/88</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.615001</td>\n",
       "      <td>261404</td>\n",
       "      <td>31.251233</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>South America</td>\n",
       "      <td>Southern Hemisphere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CZE1114</td>\n",
       "      <td>21</td>\n",
       "      <td>Male</td>\n",
       "      <td>389</td>\n",
       "      <td>165/93</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4.963459</td>\n",
       "      <td>285768</td>\n",
       "      <td>27.194973</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Canada</td>\n",
       "      <td>North America</td>\n",
       "      <td>Northern Hemisphere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BNI9906</td>\n",
       "      <td>21</td>\n",
       "      <td>Female</td>\n",
       "      <td>324</td>\n",
       "      <td>174/99</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.463426</td>\n",
       "      <td>235282</td>\n",
       "      <td>28.176571</td>\n",
       "      <td>587</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>France</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Northern Hemisphere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JLN3497</td>\n",
       "      <td>84</td>\n",
       "      <td>Male</td>\n",
       "      <td>383</td>\n",
       "      <td>163/100</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.648981</td>\n",
       "      <td>125640</td>\n",
       "      <td>36.464704</td>\n",
       "      <td>378</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Canada</td>\n",
       "      <td>North America</td>\n",
       "      <td>Northern Hemisphere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GFO8847</td>\n",
       "      <td>66</td>\n",
       "      <td>Male</td>\n",
       "      <td>318</td>\n",
       "      <td>91/88</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.514821</td>\n",
       "      <td>160555</td>\n",
       "      <td>21.809144</td>\n",
       "      <td>231</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Northern Hemisphere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Patient ID  Age     Sex  Cholesterol Blood Pressure  Heart Rate  Diabetes  \\\n",
       "0    BMW7812   67    Male          208         158/88          72         0   \n",
       "1    CZE1114   21    Male          389         165/93          98         1   \n",
       "2    BNI9906   21  Female          324         174/99          72         1   \n",
       "3    JLN3497   84    Male          383        163/100          73         1   \n",
       "4    GFO8847   66    Male          318          91/88          93         1   \n",
       "\n",
       "   Family History  Smoking  Obesity  ...  Sedentary Hours Per Day  Income  \\\n",
       "0               0        1        0  ...                 6.615001  261404   \n",
       "1               1        1        1  ...                 4.963459  285768   \n",
       "2               0        0        0  ...                 9.463426  235282   \n",
       "3               1        1        0  ...                 7.648981  125640   \n",
       "4               1        1        1  ...                 1.514821  160555   \n",
       "\n",
       "         BMI  Triglycerides  Physical Activity Days Per Week  \\\n",
       "0  31.251233            286                                0   \n",
       "1  27.194973            235                                1   \n",
       "2  28.176571            587                                4   \n",
       "3  36.464704            378                                3   \n",
       "4  21.809144            231                                1   \n",
       "\n",
       "   Sleep Hours Per Day    Country      Continent           Hemisphere  \\\n",
       "0                    6  Argentina  South America  Southern Hemisphere   \n",
       "1                    7     Canada  North America  Northern Hemisphere   \n",
       "2                    4     France         Europe  Northern Hemisphere   \n",
       "3                    4     Canada  North America  Northern Hemisphere   \n",
       "4                    5   Thailand           Asia  Northern Hemisphere   \n",
       "\n",
       "   Heart Attack Risk  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Dataset \n",
    "df = pd.read_csv('heart_attack_prediction_dataset.csv')\n",
    "\n",
    "# Printing the dataset \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8763 entries, 0 to 8762\n",
      "Data columns (total 26 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   Patient ID                       8763 non-null   object \n",
      " 1   Age                              8763 non-null   int64  \n",
      " 2   Sex                              8763 non-null   object \n",
      " 3   Cholesterol                      8763 non-null   int64  \n",
      " 4   Blood Pressure                   8763 non-null   object \n",
      " 5   Heart Rate                       8763 non-null   int64  \n",
      " 6   Diabetes                         8763 non-null   int64  \n",
      " 7   Family History                   8763 non-null   int64  \n",
      " 8   Smoking                          8763 non-null   int64  \n",
      " 9   Obesity                          8763 non-null   int64  \n",
      " 10  Alcohol Consumption              8763 non-null   int64  \n",
      " 11  Exercise Hours Per Week          8763 non-null   float64\n",
      " 12  Diet                             8763 non-null   object \n",
      " 13  Previous Heart Problems          8763 non-null   int64  \n",
      " 14  Medication Use                   8763 non-null   int64  \n",
      " 15  Stress Level                     8763 non-null   int64  \n",
      " 16  Sedentary Hours Per Day          8763 non-null   float64\n",
      " 17  Income                           8763 non-null   int64  \n",
      " 18  BMI                              8763 non-null   float64\n",
      " 19  Triglycerides                    8763 non-null   int64  \n",
      " 20  Physical Activity Days Per Week  8763 non-null   int64  \n",
      " 21  Sleep Hours Per Day              8763 non-null   int64  \n",
      " 22  Country                          8763 non-null   object \n",
      " 23  Continent                        8763 non-null   object \n",
      " 24  Hemisphere                       8763 non-null   object \n",
      " 25  Heart Attack Risk                8763 non-null   int64  \n",
      "dtypes: float64(3), int64(16), object(7)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Info about dataset \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setting column 'Blood Pressure' \n",
    "Splitting Between Diastolic and Systolic Blood Pressure\"\"\"\n",
    "\n",
    "df['BP_Systolic'] = df['Blood Pressure'].apply(lambda x: x.split('/')[0])\n",
    "df['BP_Diastolic'] = df['Blood Pressure'].apply(lambda x: x.split('/')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting object(here category present) into integer \n",
    "mapping = {\"Unhealthy\": -1, \"Average\": 0, \"Healthy\": 1}\n",
    "df[\"Diet\"] = df[\"Diet\"].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting object(here category present) into integer \n",
    "mapping = {\"Male\": -1, \"Female\": 0}\n",
    "df[\"Sex\"] = df[\"Sex\"].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Converting 'Object' and 'Boolean' Datatype into int\"\"\"\n",
    "cat_columns = ['BP_Systolic','BP_Diastolic']\n",
    "df[cat_columns] = df[cat_columns].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Patient ID                          object\n",
       "Age                                  int64\n",
       "Sex                                  int64\n",
       "Cholesterol                          int64\n",
       "Blood Pressure                      object\n",
       "Heart Rate                           int64\n",
       "Diabetes                             int64\n",
       "Family History                       int64\n",
       "Smoking                              int64\n",
       "Obesity                              int64\n",
       "Alcohol Consumption                  int64\n",
       "Exercise Hours Per Week            float64\n",
       "Diet                                 int64\n",
       "Previous Heart Problems              int64\n",
       "Medication Use                       int64\n",
       "Stress Level                         int64\n",
       "Sedentary Hours Per Day            float64\n",
       "Income                               int64\n",
       "BMI                                float64\n",
       "Triglycerides                        int64\n",
       "Physical Activity Days Per Week      int64\n",
       "Sleep Hours Per Day                  int64\n",
       "Country                             object\n",
       "Continent                           object\n",
       "Hemisphere                          object\n",
       "Heart Attack Risk                    int64\n",
       "BP_Systolic                          int64\n",
       "BP_Diastolic                         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again check the datatypes of the columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8763 entries, 0 to 8762\n",
      "Data columns (total 22 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   Age                              8763 non-null   int64  \n",
      " 1   Sex                              8763 non-null   int64  \n",
      " 2   Cholesterol                      8763 non-null   int64  \n",
      " 3   Heart Rate                       8763 non-null   int64  \n",
      " 4   Diabetes                         8763 non-null   int64  \n",
      " 5   Family History                   8763 non-null   int64  \n",
      " 6   Smoking                          8763 non-null   int64  \n",
      " 7   Obesity                          8763 non-null   int64  \n",
      " 8   Alcohol Consumption              8763 non-null   int64  \n",
      " 9   Exercise Hours Per Week          8763 non-null   float64\n",
      " 10  Diet                             8763 non-null   int64  \n",
      " 11  Previous Heart Problems          8763 non-null   int64  \n",
      " 12  Medication Use                   8763 non-null   int64  \n",
      " 13  Stress Level                     8763 non-null   int64  \n",
      " 14  Sedentary Hours Per Day          8763 non-null   float64\n",
      " 15  Income                           8763 non-null   int64  \n",
      " 16  BMI                              8763 non-null   float64\n",
      " 17  Triglycerides                    8763 non-null   int64  \n",
      " 18  Physical Activity Days Per Week  8763 non-null   int64  \n",
      " 19  Sleep Hours Per Day              8763 non-null   int64  \n",
      " 20  BP_Systolic                      8763 non-null   int64  \n",
      " 21  BP_Diastolic                     8763 non-null   int64  \n",
      "dtypes: float64(3), int64(19)\n",
      "memory usage: 1.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Removing the target column from the features set\n",
    "X = df.drop(['Patient ID', 'Blood Pressure', 'Country', 'Continent', 'Hemisphere', 'Heart Attack Risk'], axis = 1)\n",
    "y = df['Heart Attack Risk']\n",
    "print(X.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      " Heart Attack Risk\n",
      "0    5624\n",
      "1    3139\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution(to check if my class distribution is balanced or not)\n",
    "class_distribution = df['Heart Attack Risk'].value_counts()\n",
    "print(\"Class Distribution:\\n\", class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample the minority class using SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_oversampled, y_oversampled = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart Attack Risk\n",
      "0    5624\n",
      "1    5624\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Again check the distribution of class\n",
    "check_distribution = y_oversampled.value_counts()\n",
    "print(check_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data into train set and test set \n",
    "X_train , X_test , y_train , y_test = train_test_split(X_oversampled , y_oversampled ,test_size=0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling to standarize the features(such that mean is zero and variance is one.)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the feature selection\n",
    "selector = SelectFromModel(xgb.XGBClassifier()).fit(X_train, y_train)\n",
    "selected_features = selector.get_support()\n",
    "X_train_selected = X_train[:, selected_features]\n",
    "X_test_selected = X_test[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8763 entries, 0 to 8762\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Age                      8763 non-null   int64  \n",
      " 1   Heart Rate               8763 non-null   int64  \n",
      " 2   Diabetes                 8763 non-null   int64  \n",
      " 3   Family History           8763 non-null   int64  \n",
      " 4   Smoking                  8763 non-null   int64  \n",
      " 5   Alcohol Consumption      8763 non-null   int64  \n",
      " 6   Exercise Hours Per Week  8763 non-null   float64\n",
      " 7   Diet                     8763 non-null   int64  \n",
      "dtypes: float64(1), int64(7)\n",
      "memory usage: 547.8 KB\n",
      "None\n",
      "Class-Distribution\n",
      " Heart Attack Risk\n",
      "0    5624\n",
      "1    5624\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Creating New feature lists according to top 10 features only (provided in the last of the notebook)[For XGboost/RandomForest only]\n",
    "X_new = df.drop(['Patient ID', 'Blood Pressure', 'Country', 'Continent', 'Hemisphere', 'Heart Attack Risk','Sex','Cholesterol','Obesity','Previous Heart Problems','Medication Use','Stress Level','Sedentary Hours Per Day','Income','BMI','Triglycerides','Physical Activity Days Per Week','Sleep Hours Per Day','BP_Systolic','BP_Diastolic'], axis = 1)\n",
    "y_new = df['Heart Attack Risk']\n",
    "print(X_new.info())\n",
    "\n",
    "# Oversample the minority class using SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_new_oversampled, y_new_oversampled = sm.fit_resample(X_new, y_new)\n",
    "\n",
    "# Again check the distribution of class\n",
    "check_distribution = y_new_oversampled.value_counts()\n",
    "print(\"Class-Distribution\\n\",check_distribution)\n",
    "\n",
    "# Splitting Data into train set and test set \n",
    "X_new_train , X_new_test , y_new_train , y_new_test = train_test_split(X_new_oversampled , y_new_oversampled ,test_size=0.2, random_state= 42)\n",
    "\n",
    "# Feature Scaling to standarize the features(such that mean is zero and variance is one.)\n",
    "scaler = StandardScaler()\n",
    "X_new_train = scaler.fit_transform(X_new_train)\n",
    "X_new_test = scaler.transform(X_new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optuna' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Run Optuna to find the best hyperparameters\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m study \u001b[38;5;241m=\u001b[39m \u001b[43moptuna\u001b[49m\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optuna' is not defined"
     ]
    }
   ],
   "source": [
    "# Making the XGboost model for our heart attack risk prediction (Model-1) [Train with selected features]\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 40),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5)\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Run Optuna to find the best hyperparameters\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = xgb.XGBClassifier(**best_params)\n",
    "model.fit(X_train_selected, y_train)\n",
    "y_pred = model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy with tuned hyperparameters:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-06 22:05:59,257] A new study created in memory with name: no-name-e087df2f-b458-48b1-b703-038f05c1d981\n",
      "[I 2024-01-06 22:06:03,727] Trial 0 finished with value: 0.6528888888888889 and parameters: {'max_depth': 8, 'n_estimators': 455, 'random_state': 42}. Best is trial 0 with value: 0.6528888888888889.\n",
      "[I 2024-01-06 22:06:06,331] Trial 1 finished with value: 0.6626666666666666 and parameters: {'max_depth': 10, 'n_estimators': 239, 'random_state': 42}. Best is trial 1 with value: 0.6626666666666666.\n",
      "[I 2024-01-06 22:06:07,845] Trial 2 finished with value: 0.64 and parameters: {'max_depth': 4, 'n_estimators': 281, 'random_state': 45}. Best is trial 1 with value: 0.6626666666666666.\n",
      "[I 2024-01-06 22:06:11,059] Trial 3 finished with value: 0.6551111111111111 and parameters: {'max_depth': 8, 'n_estimators': 358, 'random_state': 44}. Best is trial 1 with value: 0.6626666666666666.\n",
      "[I 2024-01-06 22:06:11,671] Trial 4 finished with value: 0.6368888888888888 and parameters: {'max_depth': 4, 'n_estimators': 114, 'random_state': 41}. Best is trial 1 with value: 0.6626666666666666.\n",
      "[I 2024-01-06 22:06:12,965] Trial 5 finished with value: 0.6346666666666667 and parameters: {'max_depth': 4, 'n_estimators': 246, 'random_state': 45}. Best is trial 1 with value: 0.6626666666666666.\n",
      "[I 2024-01-06 22:06:15,749] Trial 6 finished with value: 0.6475555555555556 and parameters: {'max_depth': 7, 'n_estimators': 357, 'random_state': 45}. Best is trial 1 with value: 0.6626666666666666.\n",
      "[I 2024-01-06 22:06:18,017] Trial 7 finished with value: 0.6511111111111111 and parameters: {'max_depth': 9, 'n_estimators': 222, 'random_state': 44}. Best is trial 1 with value: 0.6626666666666666.\n",
      "[I 2024-01-06 22:06:18,663] Trial 8 finished with value: 0.6284444444444445 and parameters: {'max_depth': 3, 'n_estimators': 143, 'random_state': 41}. Best is trial 1 with value: 0.6626666666666666.\n",
      "[I 2024-01-06 22:06:20,874] Trial 9 finished with value: 0.6431111111111111 and parameters: {'max_depth': 5, 'n_estimators': 327, 'random_state': 40}. Best is trial 1 with value: 0.6626666666666666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from Randomforest with tuned hyperparameters: 0.6626666666666666\n"
     ]
    }
   ],
   "source": [
    "# Making the RandomForest model for our heart attack risk prediction (Model-2)\n",
    "def objective1(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'random_state': trial.suggest_int('random_state', 40, 45)\n",
    "    }\n",
    "\n",
    "    rf_model = RandomForestClassifier(**params)  # Pass individual hyperparameters\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_pred = rf_model.predict(X_test)\n",
    "    rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "    return rf_accuracy\n",
    "\n",
    "# Run Optuna to find the best hyperparameters\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective1, n_trials=10)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model1 = RandomForestClassifier(**best_params)  # Pass individual hyperparameters\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred = model1.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy from Randomforest with tuned hyperparameters:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8998, 22)\n",
      "y_train shape: (8998,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "# print(\"sample_weight shape:\", sample_weight_array.shape)\n",
    "sample_weights = np.ones(y_train.shape[0])\n",
    "# Calculate sample weights based on the class distribution\n",
    "class_counts = np.bincount(y_train)\n",
    "sample_weights[y_train == 1] = class_counts[0] / class_counts[1]\n",
    "\n",
    "# Normalize the sample weights\n",
    "sample_weights /= np.sum(sample_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 22:02:16.957781: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-06 22:02:17.124854: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-06 22:02:17.125333: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-06 22:02:17.126977: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-06 22:02:17.127316: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-06 22:02:17.127644: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-06 22:02:17.254137: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-06 22:02:17.255124: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-06 22:02:17.255378: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-06 22:02:17.255547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2291 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankur/.local/lib/python3.10/site-packages/keras/src/backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "2024-01-06 22:02:18.930820: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-06 22:02:20.544213: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fde692288a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-06 22:02:20.544276: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2024-01-06 22:02:20.553550: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-06 22:02:20.582698: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1704558740.662751   18652 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 6s 6ms/step - loss: 8.5122e-05 - accuracy: 0.5553 - val_loss: 0.6574 - val_accuracy: 0.6053\n",
      "Epoch 2/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 7.5739e-05 - accuracy: 0.5922 - val_loss: 0.6452 - val_accuracy: 0.6080\n",
      "Epoch 3/100\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 7.3584e-05 - accuracy: 0.6111 - val_loss: 0.6342 - val_accuracy: 0.6187\n",
      "Epoch 4/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 7.1902e-05 - accuracy: 0.6197 - val_loss: 0.6273 - val_accuracy: 0.6267\n",
      "Epoch 5/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 7.1143e-05 - accuracy: 0.6291 - val_loss: 0.6261 - val_accuracy: 0.6302\n",
      "Epoch 6/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 6.9933e-05 - accuracy: 0.6414 - val_loss: 0.6206 - val_accuracy: 0.6360\n",
      "Epoch 7/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 6.9090e-05 - accuracy: 0.6436 - val_loss: 0.6157 - val_accuracy: 0.6413\n",
      "Epoch 8/100\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 6.8874e-05 - accuracy: 0.6433 - val_loss: 0.6160 - val_accuracy: 0.6476\n",
      "Epoch 9/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 6.8440e-05 - accuracy: 0.6510 - val_loss: 0.6136 - val_accuracy: 0.6533\n",
      "Epoch 10/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 6.8139e-05 - accuracy: 0.6514 - val_loss: 0.6111 - val_accuracy: 0.6498\n",
      "Epoch 11/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 6.7522e-05 - accuracy: 0.6587 - val_loss: 0.6100 - val_accuracy: 0.6516\n",
      "Epoch 12/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 6.7552e-05 - accuracy: 0.6558 - val_loss: 0.6093 - val_accuracy: 0.6502\n",
      "Epoch 13/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 6.6979e-05 - accuracy: 0.6677 - val_loss: 0.6119 - val_accuracy: 0.6560\n",
      "Epoch 14/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 6.6928e-05 - accuracy: 0.6624 - val_loss: 0.6125 - val_accuracy: 0.6556\n",
      "Epoch 15/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 6.6377e-05 - accuracy: 0.6651 - val_loss: 0.6080 - val_accuracy: 0.6618\n",
      "Epoch 16/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 6.6193e-05 - accuracy: 0.6703 - val_loss: 0.6085 - val_accuracy: 0.6551\n",
      "Epoch 17/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 6.5693e-05 - accuracy: 0.6788 - val_loss: 0.6109 - val_accuracy: 0.6640\n",
      "Epoch 18/100\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 6.5300e-05 - accuracy: 0.6794 - val_loss: 0.6100 - val_accuracy: 0.6542\n",
      "Epoch 19/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 6.4779e-05 - accuracy: 0.6847 - val_loss: 0.6144 - val_accuracy: 0.6498\n",
      "Epoch 20/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 6.5082e-05 - accuracy: 0.6819 - val_loss: 0.6114 - val_accuracy: 0.6569\n",
      "\n",
      "Test accuracy: 65.68889021873474\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.6568888888888889\n",
      "Precision: 0.7193627450980392\n",
      "Recall: 0.5194690265486726\n",
      "F1 score: 0.6505089250000604\n"
     ]
    }
   ],
   "source": [
    "# Making the neural network model for our heart attack risk prediction (Model-3)\n",
    "\n",
    "model2 = Sequential([\n",
    "    Dense(units=128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(rate=0.2),  # Add dropout layer with a rate of 0.2\n",
    "    Dense(units=64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(rate=0.2),  # Add dropout layer with a rate of 0.2\n",
    "    Dense(units=32, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(rate=0.2),  # Add dropout layer with a rate of 0.2\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model2.compile(loss= BinaryCrossentropy(from_logits=True ,reduction='none') ,optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),metrics=['accuracy'],sample_weight_mode='temporal')\n",
    "model2.fit(X_train,y_train,epochs=100,batch_size=32, validation_data=(X_test, y_test), callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)],sample_weight=sample_weights)\n",
    "\n",
    "test_loss, test_acc = model2.evaluate(X_test, y_test, verbose=0)\n",
    "print('\\nTest accuracy:', test_acc*100)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model2.predict(X_test)\n",
    "y_pred_binary = np.where(y_pred > 0.5, 1, 0)  # Convert predicted probabilities to binary labels\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary, average='macro')\n",
    "\n",
    "# Print the evaluation results\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 65.68889021873474\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.6568888888888889\n",
      "Precision: 0.7193627450980392\n",
      "Recall: 0.5194690265486726\n",
      "F1 score: 0.6505089250000604\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the Neural Network Model\n",
    "test_loss, test_acc = model2.evaluate(X_test, y_test, verbose=0)\n",
    "print('\\nTest accuracy:', test_acc*100)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model2.predict(X_test)\n",
    "y_pred_binary = np.where(y_pred > 0.5, 1, 0)  # Convert predicted probabilities to binary labels\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary, average='macro')\n",
    "\n",
    "# Print the evaluation results\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.62555556 0.62666667 0.61277778 0.6364647  0.61756531]\n",
      "Mean cross-validation score: 0.6238060033351862\n",
      "Accuracy: 0.6222222222222222\n",
      "Precision: 0.6639344262295082\n",
      "Recall: 0.5017699115044247\n",
      "F1 score: 0.6168673714036617\n"
     ]
    }
   ],
   "source": [
    "# Perform 5-fold cross-validation for XGBOOST model\n",
    "scores = cross_val_score(model, X_train_selected, y_train, cv=5)\n",
    "print('Cross-validation scores:', scores)\n",
    "\n",
    "# Calculate the mean cross-validation score\n",
    "mean_score = np.mean(scores)\n",
    "print('Mean cross-validation score:', mean_score)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_selected)\n",
    "y_pred_binary = np.where(y_pred > 0.5, 1, 0)  # Convert predicted probabilities to binary labels\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary, average='macro')\n",
    "\n",
    "# Print the evaluation results\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.64055556 0.65611111 0.65111111 0.67982212 0.64035575]\n",
      "Mean cross-validation score: 0.6535911308751776\n",
      "Accuracy: 0.6644444444444444\n",
      "Precision: 0.6979936642027456\n",
      "Recall: 0.5849557522123894\n",
      "F1 score: 0.66244887553499\n"
     ]
    }
   ],
   "source": [
    "# Perform 5-fold cross-validation for Randomforest model\n",
    "scores = cross_val_score(model1, X_train, y_train, cv=5)\n",
    "print('Cross-validation scores:', scores)\n",
    "\n",
    "# Calculate the mean cross-validation score\n",
    "mean_score = np.mean(scores)\n",
    "print('Mean cross-validation score:', mean_score)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model1.predict(X_test)\n",
    "y_pred_binary = np.where(y_pred > 0.5, 1, 0)  # Convert predicted probabilities to binary labels\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary, average='macro')\n",
    "\n",
    "# Print the evaluation results\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Important Features:\n",
      "   Importance                  Feature\n",
      "5    0.209388      Alcohol Consumption\n",
      "3    0.165799           Family History\n",
      "6    0.151671  Exercise Hours Per Week\n",
      "1    0.133247               Heart Rate\n",
      "4    0.126508                  Smoking\n",
      "7    0.093092                     Diet\n",
      "0    0.085519                      Age\n",
      "2    0.034777                 Diabetes\n"
     ]
    }
   ],
   "source": [
    "#Getting the most important features while training with XGBOOST.\n",
    "\n",
    "# Assuming X_train_selected was created by indexing columns from the original DataFrame (df)\n",
    "selected_feature_indices = np.where(selector.get_support())[0]\n",
    "selected_feature_names = df.columns[selected_feature_indices]\n",
    "\n",
    "# Get feature importance scores from the model\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Create a dataframe of feature importances\n",
    "feature_importances_df = pd.DataFrame({'Importance': feature_importances, 'Feature': selected_feature_names})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importances_df = feature_importances_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Print the top 10 most important features\n",
    "print('Top 10 Most Important Features:')\n",
    "print(feature_importances_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the most important features while training with RandomForest.(When train with selected features [X_train_selected])\n",
    "\n",
    "# Assuming X_train_selected was created by indexing columns from the original DataFrame (df)\n",
    "selected_feature_indices = np.where(selector.get_support())[0]\n",
    "selected_feature_names = df.columns[selected_feature_indices]\n",
    "\n",
    "# Get feature importance scores from the model\n",
    "feature_importances = model1.feature_importances_\n",
    "\n",
    "# Create a dataframe of feature importances\n",
    "feature_importances_df = pd.DataFrame({'Importance': feature_importances, 'Feature': selected_feature_names})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importances_df = feature_importances_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Print the top 10 most important features\n",
    "print('Top 10 Most Important Features:')\n",
    "print(feature_importances_df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
